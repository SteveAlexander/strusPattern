<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<link rel="icon" type="image/ico" href="images/strus.ico" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Strus stream, a library for high performance document stream processing for Strus." />
	<meta name="keywords" content="high performance document stream processing pattern matching C++" />
	<meta name="author" content="Patrick Frey &lt;patrickpfrey (a) yahoo (dt) com&gt;" />
	<link rel="stylesheet" type="text/css" href="text-profile.css" title="Text Profile" media="all" />
	<title>Strus Stream</title>
</head>

<body>
<div id="wrap">
<div id="content">
	<h1>Strus Stream</h1>
	<p id="description">
	</p>

	<h3>Links about the topic and alternative solutions</h3>
	<p id="description">
	<ol>
	<li><a href="https://class.coursera.org/nlp/lecture">Video lectures</a> from the Stanford Natural Language Processing group
	about several aspects of NLP and information retrieval.
	</li>
	<li><a href="http://stanfordnlp.github.io/CoreNLP">CoreNLP</a>, a tool suite for different aspects of NLP,
		namely for <a href="http://nlp.stanford.edu/software/tokensregex.html">pattern matching</a>,
		licensed under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GPLv3</a>. 
		API available for Java.
	</li>
	<li>The <a href="https://uima.apache.org/ruta.html">Uima Ruta</a> workbench for rule-based text annotation,
	licensed under the <a href="http://www.apache.org/licenses">Apache license</a>.
	APIs available for Java an C++.
	</li>
	</ol>
	</p>

	<h2>Example</h2>
	<p id="description">The following example illustrates how strusStream works.
	</p>
	<h3>Example patterns defined</h3>
	<p id="description">We define the example rules defined in the following listing.
	The declarations with an identifier followed by a ':' specify the tokens defined by regular expressions. The counting ot the different token start positions defines the ordinal positions of the tokens. The number following a '^' after the name of the token defines its level, a sort of priorization (tokens covered completely by a token of a higher level are not ignored in the output and for ordinal position assignments).
	The declarations with an identifier followed by a '=' specify the patterns defined on tokens with the ordinal position as position reference for proximity range specifications.
	</p>
	<pre>
WORD ^1		:/\b\w+\b/;
SENT ^2		:/[.]/;
ABBREV ^3	: /\b[aA]bbr[\.]/ | /\b[aA]d[cjv][\.]/ | /\b[oO]bj[\.]/ | /\b[pP]seud[\.]/
		|  /\b[tT]rans[\.]/ | /\b[A-Za-z][\.][a-z][\.]/ | /\betc[\.]/ | /\bca[\.]/
		| /\b[mM]rs[\.]/ | /\b[pP]rof[\.]/ | /\b[rR]ev[\.]/ | /\b[hH]on[\.]/ | /\b[hH]rs[\.]/
		| /\b[A-Za-z][btlsr][\.]/ | /\b[gG]en[\.]/ | /\b[sS]ing[\.]/ | /\b[sS]yn[\.]/
		| /\b[aA]ve[\.]/ | /\b[d]dD]ept[\.]/ | /\b[eE]st[\.]/ | /\b[fF]ig[\.]/ | /\b[iI]nc[\.]/
		| /\b[oO]z[\.]/ | /\b[nN]o[\.]/ | /\b[sS]q[\.]/ | /\b[aA]ssn[\.]/ | /\b[tT]rans[\.]/;
ABBREV ^4	: /\bet\sal[\.]/ | /\b[rR][\.][iI][\.][pP]\b/;
URL ^5		: @([^\s/?\.#-][^\s/?\.#-]+\.)(aero|asia|biz|cat|com|coop|edu|gov|info|int|jobs|mil|mobi|museum|name|net|org|pro|tel|travel|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cu|cv|cx|cy|cz|cz|de|dj|dk|dm|do|dz|ec|ee|eg|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mn|mn|mo|mp|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|nom|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ra|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sj|sk|sl|sm|sn|so|sr|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|xyz|ye|yt|yu|za|zm|zw|arpa)@;
URL ^6		: @\b(https?://|ftp://)?([^\s/?\.#-]+\.)([^\s/?\.#-]+\.)([a-z]{2,6})(/[^\s]*)?@;
EMAIL ^7	: /\b([a-z0-9_\.-]+)@([\da-z\.-]+)\.([a-z\.]{2,6})\b/;
CAPWORD ^1	: /\b\p{Lu}\p{Ll}+\b/;

Name		= sequence( firstname=[0.75]CAPWORD, surname=CAPWORD | 2 );
Contact		= sequence_struct( SENT, Name, email=EMAIL | 10 );
	</pre>
	<h3>Example input</h3>
	<pre>
This is an example document about Prof. John Doe, contact mail@etc.com, or visit www.etc.ch.
He is responsible for development, etc..
	</pre>
	<h3>Tokenization</h3>
	<p id="description">The following output shows the tokens recognized with our set of rules defined:
	</p>
	<pre>
token WORD(1) at 1[0:4] 'This'
token CAPWORD(6) at 1[0:4] 'This'
token WORD(1) at 2[5:2] 'is'
token WORD(1) at 3[8:2] 'an'
token WORD(1) at 4[11:7] 'example'
token WORD(1) at 5[19:8] 'document'
token WORD(1) at 6[28:5] 'about'
token ABBREV(3) at 7[34:5] 'Prof.'
token WORD(1) at 8[40:4] 'John'
token CAPWORD(6) at 8[40:4] 'John'
token WORD(1) at 9[45:3] 'Doe'
token CAPWORD(6) at 9[45:3] 'Doe'
token WORD(1) at 10[50:7] 'contact'
token EMAIL(5) at 11[58:12] 'mail@etc.com'
token WORD(1) at 12[72:2] 'or'
token WORD(1) at 13[75:5] 'visit'
token URL(4) at 14[81:10] 'www.etc.ch'
token DOT(2) at 15[91:1] '.'
token WORD(1) at 16[93:2] 'He'
token CAPWORD(6) at 16[93:2] 'He'
token WORD(1) at 17[96:2] 'is'
token WORD(1) at 18[99:11] 'responsible'
token WORD(1) at 19[111:3] 'for'
token WORD(1) at 20[115:11] 'development'
token ABBREV(3) at 21[128:4] 'etc.'
token DOT(2) at 22[132:1] '.'
	</pre>

	<h3>Rule matching</h3>
	<p id="description">The following output shows the entities recognized with our set of rules (patterns of tokens) defined:
	</p>
	<pre>
result Name at 8:
        item surname at 9 [45:3] 1 'Doe'
        item firstname at 8 [40:4] 0.75 'John'

result Contact at 8:
        item email at 11 [58:12] 1 'mail@etc.com'
        item firstname at 8 [40:4] 0.75 'John'
        item surname at 9 [45:3] 1 'Doe'
	</pre>
</div>
</div>
</body>
</html>
